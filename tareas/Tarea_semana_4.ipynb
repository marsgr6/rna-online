{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù **Tarea semana 4 - Opci√≥n Low-Code: Imputaci√≥n de valores faltantes y modelado de series temporales**\n",
        "\n",
        "## üìå **Descripci√≥n del dataset**\n",
        "\n",
        "El conjunto de datos corresponde a mediciones de calidad del aire recogidas en una estaci√≥n en Italia, con registros horarios entre marzo de 2004 y abril de 2005.\n",
        "Las variables principales son:\n",
        "\n",
        "| N¬∞ | Variable      | Descripci√≥n                                                               |\n",
        "| -- | ------------- | ------------------------------------------------------------------------- |\n",
        "| 2  | CO(GT)        | Concentraci√≥n verdadera de CO (mg/m¬≥, referencia)                         |\n",
        "| 3  | PT08.S1(CO)   | Respuesta del sensor de √≥xido de esta√±o (target: CO)                      |\n",
        "| 4  | NMHC(GT)      | Concentraci√≥n verdadera de hidrocarburos no met√°nicos (Œºg/m¬≥, referencia) |\n",
        "| 5  | C6H6(GT)      | Concentraci√≥n verdadera de benceno (Œºg/m¬≥, referencia)                    |\n",
        "| 6  | PT08.S2(NMHC) | Respuesta del sensor de titania (target: NMHC)                            |\n",
        "| 7  | NOx(GT)       | Concentraci√≥n verdadera de NOx (ppb, referencia)                          |\n",
        "| 8  | PT08.S3(NOx)  | Respuesta del sensor de √≥xido de tungsteno (target: NOx)                  |\n",
        "| 9  | NO2(GT)       | Concentraci√≥n verdadera de NO2 (Œºg/m¬≥, referencia)                        |\n",
        "| 10 | PT08.S4(NO2)  | Respuesta del sensor de √≥xido de tungsteno (target: NO2)                  |\n",
        "| 11 | PT08.S5(O3)   | Respuesta del sensor de √≥xido de indio (target: O3)                       |\n",
        "| 12 | T             | Temperatura (¬∞C)                                                          |\n",
        "| 13 | RH            | Humedad relativa (%)                                                      |\n",
        "| 14 | AH            | Humedad absoluta                                                          |\n",
        "\n",
        "\n",
        "## üéØ **Prop√≥sito de la tarea**\n",
        "\n",
        "La tarea tiene dos objetivos principales:\n",
        "\n",
        "### 1Ô∏è‚É£ **Imputaci√≥n de valores faltantes**\n",
        "\n",
        "\n",
        "* Simular valores faltantes en el dataset mediante dos enfoques:\n",
        "\n",
        "  * **Aleatorio (at random)**: Se eliminan valores de forma dispersa.\n",
        "  * **En bloques (in blocks)**: Se eliminan valores en secuencias consecutivas para simular fallos de sensores.\n",
        "* Aplicar un m√©todo de imputaci√≥n (por ejemplo, interpolaci√≥n, modelo simple, t√©cnica autom√°tica de la biblioteca proporcionada) para completar los datos.\n",
        "\n",
        "### 2Ô∏è‚É£ **Modelado de serie temporal**\n",
        "\n",
        "\n",
        "* Seleccionar una variable adecuada para modelar con un **LSTM** o un **Transformer**.\n",
        "* La variable recomendada es:\n",
        "  **CO(GT)** (Concentraci√≥n de mon√≥xido de carbono en mg/m¬≥)\n",
        "  üëâ **Justificaci√≥n:** CO(GT) es una variable con patrones estacionales y tendencias temporales claras, es de inter√©s ambiental y presenta correlaci√≥n con otras mediciones del dataset.\n",
        "* Desarrollar un modelo b√°sico (low-code) para predecir el valor futuro de CO(GT), usando las variables disponibles como entradas y evaluando el desempe√±o del modelo.\n",
        "\n",
        "\n",
        "## ‚öôÔ∏è **Instrucciones**\n",
        "\n",
        "* Esta es una tarea **low-code**: Se proporcionar√°n plantillas de c√≥digo o scripts b√°sicos, enf√≥cate en:\n",
        "\n",
        "  * Comprender el proceso de imputaci√≥n y modelado.\n",
        "  * Ejecutar el flujo de trabajo.\n",
        "  * Interpretar y discutir los resultados: calidad de la imputaci√≥n y precisi√≥n del modelo de predicci√≥n.\n",
        "* Analizar las m√©tricas (MAE, MSE, RMSE, R¬≤) para la imputaci√≥n y la predicci√≥n.\n",
        "\n",
        "\n",
        "## ‚úÖ **Entregables**\n",
        "\n",
        "* Un breve informe que incluya:\n",
        "\n",
        "  * Gr√°ficos de la serie original, con y sin imputaci√≥n.\n",
        "  * M√©tricas de evaluaci√≥n de la imputaci√≥n.\n",
        "  * Gr√°ficos y m√©tricas del modelo LSTM o Transformer (por ejemplo: RMSE en test).\n",
        "  * Reflexi√≥n sobre la calidad de la imputaci√≥n y el modelo.\n",
        "\n",
        "## üéØ Generaci√≥n de valores faltantes aleatorios (Missing At Blocks)\n",
        "\n",
        "En esta parte se introduce un 20% de valores faltantes en el dataset diario en forma de bloques en cada columna. Esto simula errores aislados en la medici√≥n, t√≠picos de fallos espor√°dicos en los sensores.\n",
        "\n",
        "```python\n",
        "# ----------------------------------------\n",
        "# üìå IMPORTAR LIBRER√çAS\n",
        "# ----------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå CARGAR EL DATASET\n",
        "# ----------------------------------------\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/marsgr6/rna-online/refs/heads/main/data/AirQualityUCI.csv')\n",
        "\n",
        "# Combina fecha y hora en un solo √≠ndice de tipo datetime\n",
        "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], errors='coerce')\n",
        "df = df.drop(columns=['Date', 'Time'])\n",
        "df = df.set_index('DateTime')\n",
        "\n",
        "# Convierte los datos a num√©rico y reemplaza valores negativos por 0\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df[df < 0] = 0\n",
        "\n",
        "# Elimina la columna NMHC(GT) por calidad de datos\n",
        "if 'NMHC(GT)' in df.columns:\n",
        "    df = df.drop(columns=['NMHC(GT)'])\n",
        "\n",
        "# Resamplea a datos diarios calculando la media\n",
        "df_daily = df.resample('D').mean()\n",
        "df_original = df_daily.copy()\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå INTRODUCIR VALORES FALTANTES ALEATORIAMENTE POR COLUMNA\n",
        "# ----------------------------------------\n",
        "def introduce_missing_per_column(data, frac=0.2):\n",
        "    \"\"\"\n",
        "    Introduce valores faltantes de forma aleatoria por columna.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - data: DataFrame de entrada\n",
        "    - frac: Fracci√≥n de datos a eliminar por columna\n",
        "    \n",
        "    Retorna:\n",
        "    - data_missing: DataFrame con valores faltantes introducidos\n",
        "    - nan_mask: M√°scara booleana que marca los NaN introducidos\n",
        "    \"\"\"\n",
        "    data_missing = data.copy()\n",
        "    nan_mask = pd.DataFrame(False, index=data.index, columns=data.columns)  # M√°scara para rastrear NaNs\n",
        "    np.random.seed(42)  # Fijar semilla para reproducibilidad\n",
        "\n",
        "    for column in data.columns:\n",
        "        n_total = len(data[column])\n",
        "        n_missing = int(n_total * frac)  # Cantidad de valores a eliminar\n",
        "        missing_positions = np.random.choice(n_total, n_missing, replace=False)  # √çndices aleatorios\n",
        "        # Introducir NaN\n",
        "        data_missing.iloc[missing_positions, data.columns.get_loc(column)] = np.nan\n",
        "        nan_mask.iloc[missing_positions, data.columns.get_loc(column)] = True\n",
        "\n",
        "    print(f\"Number of missing values introduced: {nan_mask.sum().sum()}\")\n",
        "    return data_missing, nan_mask\n",
        "\n",
        "def introduce_missing_blocks(data, frac=0.2, block_size=5):\n",
        "    \"\"\"\n",
        "    Introduce missing data in contiguous blocks.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame\n",
        "    - frac: Fraction of data points to set as NaN\n",
        "    - block_size: Number of consecutive rows in each missing block\n",
        "\n",
        "    Returns:\n",
        "    - data_missing: DataFrame with missing values\n",
        "    - nan_mask: Boolean DataFrame where True = missing position introduced\n",
        "    \"\"\"\n",
        "    data_missing = data.copy()\n",
        "    nan_mask = pd.DataFrame(False, index=data.index, columns=data.columns)\n",
        "\n",
        "    n_total = len(data)\n",
        "    n_blocks_per_col = int((n_total * frac) / block_size)\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    for col in data.columns:\n",
        "        for _ in range(n_blocks_per_col):\n",
        "            start_idx = np.random.randint(0, n_total - block_size + 1)\n",
        "            block_idx = data.index[start_idx : start_idx + block_size]\n",
        "\n",
        "            data_missing.loc[block_idx, col] = np.nan\n",
        "            nan_mask.loc[block_idx, col] = True\n",
        "\n",
        "    print(f\"Number of missing values introduced: {nan_mask.sum().sum()}\")\n",
        "    return data_missing, nan_mask\n",
        "\n",
        "df_missing, nan_mask = introduce_missing_blocks(df_daily, frac=0.2, block_size=5)\n",
        "\n",
        "# Visualize\n",
        "import missingno as msno\n",
        "msno.matrix(df_missing, figsize=(12, 5), fontsize=12)\n",
        "plt.title(\"Matriz de valores faltantes en bloques por columna\")\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PLOT SERIES TEMPORALES EN 2 COLUMNAS (CON PUNTOS ROJOS PARA MISSING)\n",
        "# ----------------------------------------\n",
        "cols = df_missing.columns\n",
        "n_cols = 2\n",
        "n_rows = int(np.ceil(len(cols) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 2 * n_rows), sharex=True)\n",
        "\n",
        "axes = axes.reshape(n_rows, n_cols)\n",
        "\n",
        "for i, col in enumerate(cols):\n",
        "    ax = axes[i // n_cols, i % n_cols]\n",
        "\n",
        "    # Whole series as a blue line\n",
        "    ax.plot(df_missing.index, df_original[col], color='blue', alpha=0.7, label='Original')\n",
        "\n",
        "    # Red dots where missing\n",
        "    ax.plot(df_missing.index[df_missing[col].isna()],\n",
        "            df_original[col][df_missing[col].isna()],\n",
        "            'r.', label='Missing', markersize=6)\n",
        "\n",
        "    ax.set_title(f\"{col}\")\n",
        "    ax.set_ylabel(col)\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(i + 1, n_rows * n_cols):\n",
        "    fig.delaxes(axes[j // n_cols, j % n_cols])\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='upper center', ncol=2)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.suptitle(\"Daily Time Series with Missing Data Highlighted\", fontsize=16)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### ‚úÖ Lo que se espera\n",
        "\n",
        "- Ejecutar el c√≥digo y analizar los gr√°ficos generados.\n",
        "\n",
        "- Reflexionar: ¬øC√≥mo se distribuyen los valores faltantes aleatorios? ¬øQu√© impacto tendr√≠a esto en la imputaci√≥n?\n",
        "\n",
        "## üéØ Imputaci√≥n de valores faltantes con SAITS\n",
        "\n",
        "- Requistos\n",
        "\n",
        "```Pyton\n",
        "!pip install pypots==0.11\n",
        "```\n",
        "\n",
        "En esta secci√≥n se utiliza el modelo **SAITS** (Self-Attention-based Imputation for Time Series) del paquete `pypots` para imputar los valores faltantes generados previamente.  \n",
        "El modelo est√° basado en transformers y es capaz de capturar dependencias temporales en las series.\n",
        "\n",
        "```python\n",
        "import missingno as msno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pypots.imputation import SAITS\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå CONFIGURACI√ìN DE PAR√ÅMETROS\n",
        "# ----------------------------------------\n",
        "seq_len = 7  # Longitud de la ventana temporal (por ejemplo, 7 d√≠as)\n",
        "n_features = len(df_missing.columns)  # N√∫mero de variables (columnas)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PREPARAR LOS DATOS PARA SAITS\n",
        "# ----------------------------------------\n",
        "# Convertir DataFrame a array numpy\n",
        "data = df_missing.to_numpy(dtype=np.float32)\n",
        "\n",
        "# Generar ventanas deslizantes de longitud seq_len\n",
        "n_samples = len(data) - seq_len + 1\n",
        "X = np.array([data[i:i + seq_len] for i in range(n_samples)])\n",
        "print(\"Shape de X:\", X.shape)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå AJUSTAR X PARA QUE COINCIDA CON EL N√öMERO DE D√çAS ORIGINALES\n",
        "# ----------------------------------------\n",
        "# Repetir y rellenar filas para cubrir todos los d√≠as\n",
        "repeat_factor = data.shape[0] // X.shape[0]\n",
        "extra_rows = data.shape[0] % X.shape[0]\n",
        "\n",
        "expanded_arr = np.repeat(X, repeat_factor, axis=0)\n",
        "expanded_arr = np.vstack([expanded_arr, X[:extra_rows]])\n",
        "\n",
        "# Asegurar que las √∫ltimas filas coincidan con los datos originales\n",
        "expanded_arr[-extra_rows:, 0, :] = data[-extra_rows:]\n",
        "print(\"Shape de X expandido:\", expanded_arr.shape)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå NORMALIZAR LOS DATOS\n",
        "# ----------------------------------------\n",
        "scaler = MinMaxScaler()\n",
        "X_reshaped = expanded_arr.reshape(-1, expanded_arr.shape[-1])\n",
        "X_scaled = scaler.fit_transform(X_reshaped)\n",
        "X_scaled = X_scaled.reshape(expanded_arr.shape)\n",
        "print(\"Shape de X escalado:\", X_scaled.shape)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå ENTRENAR EL MODELO SAITS\n",
        "# ----------------------------------------\n",
        "saits = SAITS(n_steps=seq_len, n_features=n_features,\n",
        "              n_layers=2, d_model=256, d_ffn=128,\n",
        "              n_heads=4, d_k=64, d_v=64, dropout=0.1, epochs=100)\n",
        "\n",
        "dataset = {\"X\": X_scaled}\n",
        "saits.fit(dataset)  # Entrenar el modelo\n",
        "imputation = saits.impute(dataset)  # Imputar los valores faltantes\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå DESNORMALIZAR LA IMPUTACI√ìN\n",
        "# ----------------------------------------\n",
        "imputation_reshaped = imputation.reshape(-1, imputation.shape[-1])\n",
        "imputation_denorm = scaler.inverse_transform(imputation_reshaped)\n",
        "imputation_denorm = imputation_denorm.reshape(imputation.shape)\n",
        "\n",
        "# Tomar las primeras posiciones imputadas de cada ventana\n",
        "imputed_values = imputation_denorm[:, 0, :]\n",
        "print(\"Shape de los datos imputados finales:\", imputed_values.shape)\n",
        "\n",
        "# Reconstruir el DataFrame con los valores imputados\n",
        "data_imputed = pd.DataFrame(imputed_values, columns=df.columns, index=df_original.index[:imputed_values.shape[0]])\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå VISUALIZAR ORIGINAL VS IMPUTADO\n",
        "# ----------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    int(np.ceil(len(df_original.columns) / 2)), 2, figsize=(16, 2 * int(np.ceil(len(df_original.columns) / 2)))\n",
        ")\n",
        "\n",
        "for ax, col in zip(axes.flat, df_original.columns):\n",
        "    ax.plot(df_original.index, df_original[col], label=\"Original\", color='blue')\n",
        "    ax.plot(df_original.index, data_imputed[col], ':', label=\"Imputed\", color='orange')\n",
        "    ax.set_title(col)\n",
        "\n",
        "fig.legend(['Original', 'Imputed'], loc='upper center', ncol=2, fontsize=12)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "````\n",
        "\n",
        "## üìä Resultados de la imputaci√≥n con SAITS\n",
        "\n",
        "| Variable        | MAE    | MSE       | RMSE   | R¬≤      |\n",
        "|-----------------|--------|-----------|--------|---------|\n",
        "| CO(GT)          | 0.281  | 0.106     | 0.326  | 0.746   |\n",
        "| PT08.S1(CO)     | 61.061 | 5673.883  | 75.325 | 0.850   |\n",
        "| C6H6(GT)        | 0.995  | 1.878     | 1.370  | 0.872   |\n",
        "| PT08.S2(NMHC)   | 37.641 | 2561.053  | 50.607 | 0.929   |\n",
        "| NOx(GT)         | 48.429 | 3705.005  | 60.869 | 0.866   |\n",
        "| PT08.S3(NOx)    | 70.764 | 9218.081  | 96.011 | 0.780   |\n",
        "| NO2(GT)         | 15.125 | 357.961   | 18.920 | 0.861   |\n",
        "| PT08.S4(NO2)    | 78.496 | 10095.427 | 100.476| 0.858   |\n",
        "| PT08.S5(O3)     | 94.586 | 13372.547 | 115.640| 0.864   |\n",
        "| T               | 2.115  | 6.943     | 2.635  | 0.840   |\n",
        "| RH              | 6.708  | 65.337    | 8.083  | 0.674   |\n",
        "| AH              | 0.125  | 0.022     | 0.147  | 0.859   |\n",
        "\n",
        "\n",
        "### üí° Preguntas  \n",
        "- ¬øQu√© variables presentan la imputaci√≥n m√°s precisa seg√∫n R¬≤? ¬øPor qu√© creen que ocurre?\n",
        "- ¬øQu√© caracter√≠sticas del dataset podr√≠an dificultar la imputaci√≥n de ciertas variables?\n",
        "- ¬øEl error de imputaci√≥n ser√≠a aceptable para un an√°lisis ambiental? ¬øPor qu√©?\n",
        "\n",
        "\n",
        "## üéØ Modelado de CO(GT) usando Transformer\n",
        "\n",
        "Una vez imputados los valores faltantes, el objetivo es **modelar la serie temporal de CO(GT)** para predecir su valor futuro a partir de sus valores anteriores y/o las dem√°s variables.\n",
        "\n",
        "üëâ **Variable objetivo**: CO(GT)  \n",
        "üëâ **Justificaci√≥n**: Es una variable ambiental clave, con tendencia y estacionalidad diarias evidentes, lo que la hace adecuada para modelado secuencial.\n",
        "\n",
        "### üìå Descripci√≥n del flujo\n",
        "El modelo Transformer recibe secuencias pasadas y aprende a predecir el siguiente valor de CO(GT).\n",
        "\n",
        "### üìå C√≥digo base\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PREPARAR LOS DATOS\n",
        "# ----------------------------------------\n",
        "seq_len = 7  # Usaremos 7 d√≠as como ventana (se puede ajustar)\n",
        "\n",
        "# Tomamos la serie imputada\n",
        "series = data_imputed['CO(GT)'].to_numpy(dtype=np.float32)\n",
        "\n",
        "# Generar secuencias y etiquetas\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(series) - seq_len):\n",
        "    X.append(series[i:i + seq_len])\n",
        "    y.append(series[i + seq_len])\n",
        "\n",
        "X = np.array(X)  # (n_samples, seq_len)\n",
        "y = np.array(y)  # (n_samples,)\n",
        "\n",
        "# A√±adir dimensi√≥n de caracter√≠sticas (1 caracter√≠stica: CO)\n",
        "X = X[..., np.newaxis]  # (n_samples, seq_len, 1)\n",
        "\n",
        "# Crear dataloader\n",
        "batch_size = 16\n",
        "dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå DEFINIR EL TRANSFORMER\n",
        "# ----------------------------------------\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, seq_len, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(1, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, 1)\n",
        "        x = self.input_proj(x)  # (batch, seq_len, d_model)\n",
        "        x = x.permute(1, 0, 2)  # Transformer espera (seq_len, batch, d_model)\n",
        "        x = self.transformer(x)\n",
        "        x = x[-1]  # Tomamos la salida del √∫ltimo paso temporal\n",
        "        x = self.output(x).squeeze(1)  # (batch,)\n",
        "        return x\n",
        "\n",
        "# Instanciar el modelo\n",
        "model = SimpleTransformer(seq_len=seq_len)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå ENTRENAR\n",
        "# ----------------------------------------\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "n_epochs = 50\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(batch_X)\n",
        "        loss = loss_fn(pred, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss / len(loader):.4f}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå EVALUAR (p. ej. en todo el conjunto)\n",
        "# ----------------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(torch.from_numpy(X)).numpy()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y, label='True')\n",
        "plt.plot(y_pred, label='Predicted', linestyle=':')\n",
        "plt.legend()\n",
        "plt.title(\"Predicci√≥n de CO(GT) con Transformer\")\n",
        "plt.show()\n",
        "````\n",
        "\n",
        "### ‚úÖ Lo que debes hacer\n",
        "\n",
        "* Ejecutar el c√≥digo.\n",
        "* Observar las predicciones generadas.\n",
        "* Reflexionar: ¬øC√≥mo se ajusta el modelo a los datos? ¬øQu√© podr√≠amos mejorar (m√°s capas, regularizaci√≥n, m√°s features)?\n",
        "\n",
        "## üéØ Forecasting multivariado de CO con Transformer\n",
        "\n",
        "El objetivo es modelar y predecir el CO utilizando un Transformer multivariado.  \n",
        "üëâ El modelo aprende a partir de los datos previos y se eval√∫a su desempe√±o sobre un conjunto de test (√∫ltimo 20% del tiempo).\n",
        "\n",
        "Se graficar√° la serie completa mostrando el ajuste y se comparar√°n los valores reales y predichos en test.\n",
        "\n",
        "\n",
        "### üìå C√≥digo base\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PREPARAR LOS DATOS\n",
        "# ----------------------------------------\n",
        "seq_len = 7\n",
        "data_arr = data_imputed.to_numpy(dtype=np.float32)\n",
        "\n",
        "# Secuencias y etiquetas\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_arr) - seq_len):\n",
        "    X.append(data_arr[i:i + seq_len])\n",
        "    y.append(data_arr[i + seq_len][data_imputed.columns.get_loc('CO(GT)')])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Separar train / test (80% / 20%)\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
        "                          batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå DEFINIR EL TRANSFORMER\n",
        "# ----------------------------------------\n",
        "class ForecastTransformer(nn.Module):\n",
        "    def __init__(self, n_features, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(n_features, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.output = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)  # (batch, seq_len, d_model)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch, d_model)\n",
        "        x = self.transformer(x)\n",
        "        x = x[-1]\n",
        "        x = self.output(x).squeeze(1)\n",
        "        return x\n",
        "\n",
        "model = ForecastTransformer(n_features=X.shape[2])\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå ENTRENAR\n",
        "# ----------------------------------------\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(batch_X)\n",
        "        loss = loss_fn(pred, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PREDICCIONES\n",
        "# ----------------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = model(torch.from_numpy(X_train)).numpy()\n",
        "    y_pred_test = model(torch.from_numpy(X_test)).numpy()\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PLOT DE LA SERIE COMPLETA\n",
        "# ----------------------------------------\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(range(len(y)), y, label='True CO(GT)', color='blue')\n",
        "plt.plot(range(seq_len, seq_len + len(y_pred_train)), y_pred_train, label='Predicted Train', color='green', alpha=0.7)\n",
        "plt.plot(range(seq_len + len(y_pred_train), seq_len + len(y_pred_train) + len(y_pred_test)),\n",
        "         y_pred_test, label='Predicted Test', color='orange', linestyle=':')\n",
        "plt.axvline(seq_len + len(y_pred_train), color='gray', linestyle='--', label='Train/Test Split')\n",
        "plt.legend()\n",
        "plt.title(\"CO(GT) Forecasting - True vs Predicted (Train + Test)\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"CO(GT)\")\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------------\n",
        "# üìå PLOT COMPARATIVO TEST FINAL\n",
        "# ----------------------------------------\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y_test, label='True CO(GT)', color='blue')\n",
        "plt.plot(y_pred_test, label='Predicted CO(GT)', color='orange', linestyle=':')\n",
        "plt.legend()\n",
        "plt.title(\"CO(GT) Forecasting - Test Set Detail\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"CO(GT)\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### ‚úÖ Lo que se espera\n",
        "\n",
        "* Analizar el gr√°fico de la serie completa: ¬øc√≥mo se ajusta el modelo en train y en test?\n",
        "* Observar el detalle en test: ¬øqu√© tan bien predice el modelo los valores futuros?\n",
        "* Proponer mejoras si fuera necesario (m√°s capas, regularizaci√≥n, ajuste de hiperpar√°metros).\n",
        "\n",
        "- **Vea las recomendaciones al final de la Opci√≥n 2 para incluir posibles mejoras en el modelo.**\n",
        "\n",
        "# üìù **Opci√≥n 2 - Informe ejecutivo: Imputaci√≥n de valores faltantes y forecasting de CO(GT)**\n",
        "\n",
        "## üìå **Contexto**\n",
        "\n",
        "El trabajo se realiz√≥ sobre un conjunto de datos de calidad del aire recogido en una estaci√≥n en Italia (2004-2005), con variables como CO(GT), NOx(GT), sensores PT08.\\*, temperatura (T), humedad relativa (RH) y humedad absoluta (AH).\n",
        "\n",
        "## üìù **Instrucci√≥n para el informe ejecutivo**\n",
        "\n",
        "En esta opci√≥n de **informe ejecutivo**, el estudiante no debe centrarse en el c√≥digo ni en su ejecuci√≥n. El objetivo es que:\n",
        "\n",
        "‚úÖ **Analice cr√≠ticamente los resultados obtenidos**, bas√°ndose en:\n",
        "\n",
        "* Las figuras proporcionadas: visualizaci√≥n de valores faltantes, imputaci√≥n realizada por SAITS, y predicciones del modelo Transformer.\n",
        "* Las m√©tricas de evaluaci√≥n de la imputaci√≥n (MAE, MSE, RMSE, R¬≤ por variable).\n",
        "* Las m√©tricas del modelo de forecasting (MAE, MSE, RMSE, R¬≤ en test).\n",
        "\n",
        "‚úÖ **Discuta los aciertos y limitaciones**:\n",
        "\n",
        "* ¬øQu√© variables se imputaron mejor y por qu√©?\n",
        "* ¬øQu√© patrones se observan en las figuras de imputaci√≥n y forecasting?\n",
        "* ¬øQu√© dificultades tuvo el modelo para generalizar al conjunto de test?\n",
        "\n",
        "‚úÖ **Proponga recomendaciones para mejorar**:\n",
        "\n",
        "* Argumentar c√≥mo podr√≠an optimizarse los resultados, considerando pistas como:\n",
        "\n",
        "  * Aumentar la longitud de la ventana temporal (`seq_len`).\n",
        "  * Ajustar el n√∫mero de capas, cabezas de atenci√≥n y dimensiones del Transformer.\n",
        "  * Aplicar mayor regularizaci√≥n (dropout, weight decay).\n",
        "  * Explorar otras arquitecturas como LSTM, TCN o enfoques h√≠bridos.\n",
        "  * Incorporar variables adicionales o ex√≥genas (por ejemplo: estacionales, d√≠a de la semana).\n",
        "\n",
        "‚úÖ **Redacte un informe claro y estructurado**, que:\n",
        "\n",
        "* Incluya las figuras proporcionadas como soporte visual.\n",
        "* Presente las m√©tricas en tablas.\n",
        "* Explique los resultados en un lenguaje t√©cnico, pero accesible.\n",
        "\n",
        "### üí° **Pautas adicionales**\n",
        "\n",
        "üëâ El informe debe responder a preguntas como:\n",
        "\n",
        "* ¬øQu√© evidencian los gr√°ficos en relaci√≥n con la imputaci√≥n y el forecasting?\n",
        "* ¬øQu√© m√©tricas destacan por su buen o mal desempe√±o?\n",
        "* ¬øQu√© tan adecuados son los resultados para un an√°lisis ambiental real?\n",
        "\n",
        "üëâ Se espera un an√°lisis reflexivo y bien argumentado, que sirva como base para futuras mejoras del proceso.\n",
        "\n",
        "\n",
        "## üéØ **Generaci√≥n y visualizaci√≥n de valores faltantes**\n",
        "\n",
        "Se introdujo un 20% de valores faltantes en bloques para simular fallos de sensores.\n",
        "\n",
        "üìå **Figura: Matriz de valores faltantes en bloques**\n",
        "![Matriz de valores faltantes](https://raw.githubusercontent.com/marsgr6/r-scripts/refs/heads/master/imgs/mv_blocks_ts_4.png)\n",
        "\n",
        "üìå **Figura: Series temporales con valores faltantes resaltados**\n",
        "![Series temporales con missing](https://raw.githubusercontent.com/marsgr6/r-scripts/refs/heads/master/imgs/ts_mv_blocks_ts_4.png)\n",
        "\n",
        "**Observaciones**:\n",
        "\n",
        "* Los valores faltantes se distribuyen en tramos consecutivos en todas las variables.\n",
        "* Las series temporales muestran los huecos (en rojo) que representan los bloques de datos faltantes.\n",
        "\n",
        "## üéØ **Imputaci√≥n con SAITS**\n",
        "\n",
        "Se aplic√≥ el modelo SAITS para completar los datos.\n",
        "\n",
        "üìå **Figura: Comparaci√≥n de series originales vs imputadas**\n",
        "![Imputaci√≥n SAITS](https://raw.githubusercontent.com/marsgr6/r-scripts/refs/heads/master/imgs/ts_imputation_saits_ts_4.png)\n",
        "\n",
        "### üìä **M√©tricas de evaluaci√≥n de la imputaci√≥n**\n",
        "\n",
        "| Variable      | MAE    | MSE       | RMSE    | R¬≤    |\n",
        "| ------------- | ------ | --------- | ------- | ----- |\n",
        "| CO(GT)        | 0.281  | 0.106     | 0.326   | 0.746 |\n",
        "| PT08.S1(CO)   | 61.061 | 5673.883  | 75.325  | 0.850 |\n",
        "| C6H6(GT)      | 0.995  | 1.878     | 1.370   | 0.872 |\n",
        "| PT08.S2(NMHC) | 37.641 | 2561.053  | 50.607  | 0.929 |\n",
        "| NOx(GT)       | 48.429 | 3705.005  | 60.869  | 0.866 |\n",
        "| PT08.S3(NOx)  | 70.764 | 9218.081  | 96.011  | 0.780 |\n",
        "| NO2(GT)       | 15.125 | 357.961   | 18.920  | 0.861 |\n",
        "| PT08.S4(NO2)  | 78.496 | 10095.427 | 100.476 | 0.858 |\n",
        "| PT08.S5(O3)   | 94.586 | 13372.547 | 115.640 | 0.864 |\n",
        "| T             | 2.115  | 6.943     | 2.635   | 0.840 |\n",
        "| RH            | 6.708  | 65.337    | 8.083   | 0.674 |\n",
        "| AH            | 0.125  | 0.022     | 0.147   | 0.859 |\n",
        "\n",
        "**An√°lisis**:\n",
        "\n",
        "* Las variables como PT08.S2(NMHC), PT08.S1(CO) y C6H6(GT) presentan R¬≤ altos (>0.85), indicando una buena imputaci√≥n.\n",
        "* RH muestra menor desempe√±o (R¬≤ = 0.674), probablemente debido a mayor variabilidad o menor dependencia de otras variables.\n",
        "\n",
        "## üéØ **Forecasting multivariado de CO(GT) con Transformer**\n",
        "\n",
        "El modelo Transformer fue entrenado con las variables imputadas para predecir CO(GT).\n",
        "\n",
        "üìå **Figura: True vs Predicted (Train + Test)**\n",
        "![Transformer CO Forecasting](https://raw.githubusercontent.com/marsgr6/r-scripts/refs/heads/master/imgs/transformer_co_ts_4.png)\n",
        "\n",
        "### üìä **M√©tricas de forecasting (test set)**\n",
        "\n",
        "| M√©trica | Valor  |\n",
        "| ------- | ------ |\n",
        "| MAE     | 0.709  |\n",
        "| MSE     | 0.884  |\n",
        "| RMSE    | 0.940  |\n",
        "| R¬≤      | -0.404 |\n",
        "\n",
        "**An√°lisis**:\n",
        "\n",
        "* El R¬≤ negativo indica que el modelo no supera un predictor constante (media de la serie), sugiriendo sobreajuste o insuficiente captura de la din√°mica temporal.\n",
        "* Visualmente, el modelo sigue bien la tendencia en train, pero las predicciones en test son planas o desacopladas del patr√≥n real.\n",
        "\n",
        "## üí° **Reflexiones y propuestas de mejora**\n",
        "\n",
        "‚úÖ **SAITS**\n",
        "\n",
        "* SAITS logra buena calidad de imputaci√≥n en la mayor√≠a de variables.\n",
        "* Las variables con menor R¬≤ podr√≠an beneficiarse de una imputaci√≥n combinada (e.g. interpolaci√≥n + SAITS).\n",
        "\n",
        "‚úÖ **Transformer**\n",
        "\n",
        "* Incrementar la longitud de las secuencias (por ejemplo seq\\_len > 7).\n",
        "* A√±adir regularizaci√≥n (mayor dropout, weight decay).\n",
        "* Probar ajustes de hiperpar√°metros: m√°s capas, m√°s cabezas de atenci√≥n.\n",
        "* Explorar arquitecturas alternativas como LSTM o TCN.\n",
        "\n",
        "‚úÖ **General**\n",
        "\n",
        "* Considerar agregar variables externas (e.g. d√≠a de la semana) como input al modelo.\n",
        "\n",
        "üëâ **Siguiente paso sugerido:** Implementar un proceso de validaci√≥n cruzada y tuning de hiperpar√°metros para el Transformer."
      ],
      "metadata": {
        "id": "6FgUUNUbf6_1"
      }
    }
  ]
}
